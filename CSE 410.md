# Themes

- Simpler is Faster
- Static vs. Dynamic Evaluation
- Representation and Translation
- Interfaces vs. Implementation
  - Layers, not options
  - Policy vs. mechanism
  - Interposition to evolve functionality
- Naming / Virtualization
- Parallelism / Concurrency
- Trading space for time

## Simpler is Faster

- RISC-V ISA
  - load-store architecture (all operations are on values in registers)
  - instructions are all 32 bits long (vs. variable length instructions)
  - simple operations (load, store, add, and, shift, branch, jal, etc.)
    - no hardware notion of stack
    - no hardware notion of procedure call (beyond jal instruction)
    - no hareware notion of type
- Languages
  - C vs. Java (functionality / performance)

### Layering

- OS over hardware
  - abstracts memory to address space
  - abstracts processor to thread
  - abstracts disk to file system
  - abstracts network to sockets / connections
- Virtual memory
  - Hardware provides mechanism ( address translation and page faults)
  - OS provides policy (management of physical memory, setting access rights in virtual memory)
- Language over ISA / OS abstractions
  - programmer works to language specifications, not ISA / OS specifications  
    - more efficient for programmer
    - portable across ISAs
- ISA over hardware
  - hardware is boolean circuits, ISA is an interface specifications
- Caches / Memory Hierarchy
  - Main memory cache
  - Small / fast
  - Transparent
- Registers
  - a "programmer controlled cache"
  - pushes issue of deciding which values are most important right now to the compiler (see Static vs. Dynamic)

## Static vs. Dynamic

- We often prefer to do things statically - at compile / build time - because then we don't pay the cost of doing it at each execution
  - Type checking
- Additionally, we can "view the entire program" statically, whereas we see only the current instruction at run time
  - Compiler can perform optimizations statically that you couldn't perform dynamically
    - Dead code removal
- Some things can be done only dynamically
  - Optimizating instruction sequences across brances 
    - Inserting bubbles in pipeline when needed, but only when needed
  - All computations / optimizations involving input

## Representation and Translation

- Everything is bits
  - Why base 2? Why not base 3?
- Intergers
  - signed and unsigned (why?)
  - overflow
- Floats
- Characters
- Strings
- instructions
  - Want compact encoding of instructions. Why?
  - Must be able to encode every possible instruction

### Instructions

- Instruction formats
- Why have an immediate format instruction?
  - The instruction operand is ready as soon as the instruction has been decoded, instead of having to wait for the result of a previous memory load instruction.
- Why base-displacement memory addressing?
  - Every address is shorter
  - Addresses become correct no matter where the program is loaded in memory
- PC relative branch format
  - branch range center around PC
- Assembly language
  - "human readable" assembly language
  - Role of the assembler
    - What does it do, what doesn't it do (compared with a compiler)?

## Interfaces vs. Implementations

- ISA as an interface
  - One-at-a-time instruction execution model
- Pipelining as an Implementation
  - Many instructions in execution at once (Instruction Level Parallelism)
  - Pipelining is made more effective by careful design of the ISA (RISC-V)
- Pipeline hazards
  - Load hazards
  - Control hazards

## Layers, Not options

- Programs on top of compilers on top of OS on top of hardware
- User level threads on top of kernel threads

## Policy vs. mechanism

- Example
  - Mechanism: trap handler mechanism
  - Policy: what ever OS decides to do in response
- Example
  - Signals
  - Policy: whatever app decides to do in response
- The most general way to defer policy to a layer above is to allow that layer to execute code when an event that requires a policy decision occurs

## Interposition / Naming

- Interposition is finding an existing interface and inserting new functionality that conforms to the existing interface
- Example
  - Main memory Caches
  - Virtual Memory
  - Copy-on-write fork()

## Parallelism / Concurrency

- Instruction level Parallelism
  - Pipelines
- Processes
  - More than one application running concurrently
- Threads
  - A single application using more than one core concurrently
  - A single application dividing it's control flow into simple, relatively independent paths

## Trading Space for time

- Caches
  - More space, less time
- Virtual Memory
  - Less space, more time
- Pipelines
  - More space, less time

# Skills

## Compile

- Compile C-like code to RISC-V assembler
- Decompile RISC-V assembler to C-like code

```c++
int sub(int *P0, int P1) {
  int L1 = 0;
  for (int L0 = 0; L0 < P1; L0++) {
    if (20480 - 481 < L1) {
      return L1;
    }
    L1 += P0[L0];
  }
}
```
```
sub:
    addi    sp,sp,-48       // sp = sp + 48
    sw      s0,44(sp)       // s0 -> 44(sp)
    addi    s0,sp,48        // s0 = sp + 48
    sw      a0,-36(s0)      // save fn args / return vals to -36(s0)
    sw      a1,-40(s0)      // save fn args / return vals to -40(s0)
    sw      zero,-24(s0)    // 0 -> -24(s0)
    sw      zero,-20(s0)    // 0 -> -20(s0)
    jal     x0,.L2          // x0 = PC + 4, PC += imm
.L4:
    lw      a5,-20(s0)      // a5 <- -20(s0)
    slli    a5,a5,2         // a5 = a5 << 2
    lw      a4,-36(s0)      // a4 <- -36(s0) 
    add     a5,a4,a5        // a5 = a4 + a5
    lw      a5,0(a5)        // a5 <- 0(a5)
    lw      a4,-24(s0)      // a4 <- -24(s0)
    add     a5,a4,a5        // a5 = a4 + a5
    sw      a5,-24(s0)      // a5 -> -24(s0)
    lw      a5,-20(s0)      // a5 <- -20(s0)
    addi    a5,a5,1         // a5 = a5 + 1
    sw      a5,-20(s0)      // a5 -> -20(s0)
.L2:
    lw      a4,-20(s0)      // a4 <- -20(s0)
    lw      a5,-40(s0)      // a5 <- -40(s0)
    bge     a4,a5,.L3       // a4 >= a5 ? .L3 : nop
    lw      a4,-24(s0)      // a4 <- -24(s0)
    addi    a5,x0, 20480    // a5 = 0 + 20480
    addi    a5,a5,-481      // a5 = a5 + -481
    bge     a5, a4, .L4     // a5 >= a4 ? .L4 : nop
.L3:
    lw      a5,-24(s0)      // a5 <- -24(s0)
    addi    a0, a5, 0       // a0 = a5 + 0
    lw      s0,44(sp)       // s0 -> 44(sp)
    addi    sp,sp,48        // sp = sp + 48
    jalr    x0, ra, 0       // x0 += 4, PC = ra + 0
```

- Encode / Decode RISC-V assembler / machine instructions
- Manually simulate the result of executing a RISC-V assembler
- Follow subroutine calling conventions in RISC-V assembler to call a function and to return a value from function to the caller
  - Including caller/callee saved registers
- Manuall optimize a sequence of assembler instructions by re-writing them to an equivalnet sequence that runs in fewer cycles

## Boolean circuits

- Can define truth tables for a Boolean function
- Can convert a truth table into a Boolean circuit
- Can convert a Boolean circuit into a truth table
- Understand how Boolean circuits can do binary addition
  - half-adders and full-adders
  - [half-adder]()
  - [full-adder]()

## Machine Organziation

- Determin how many cycles it will take to issue a sequence of RISC-V instructions into the standard five stage pipeline
  - Including dealing with control and load hazards when the pipeline implements forwarding
- Determine the RAW, WAW, and WAR dependences that limit possible parallel execution of a sequence of instructions

## Caches

- Understand when caches will be effective
  - Temporal and spatial locality
- Informally evaluate code in terms of how much spatial and / or temporal locality it has
- Apply some simple re-writes to code to improve it's spatial and / or temporal locality
  - We used block matrix multiply as an example in class
- Given a cache design (block / cache line size, number of lines, and set associativity) and a memory address, determine where in the cache to look for a possibly cached copy of the value stored at that address

## OS / Security

- Understand the mechanism the OS uses to protect the CPU
- Understand the mechanism the OS uses to protect I/O devices
- Understand the mechanism the OS uses to protect memory
- Be able to explain how the OS on klaatu keeps your from running a program that reads private files owned by other users

## OS / Processes

- Understand fork / exec
  - What they do
    - Fork: Making a copy of parent process (the copy is now the child) 
    - Exec: Run the program provided in a process
  - Why giving a chance for the parent's code to run in the context of the child's process is a good idea
  - How is "inheritance" used (and useful) in fork()?
- Understand / write code for a simple shell that does input / output redirection or creates pipes between Processes
- Understand how it is that the OS protects

## Virtual Memory

- Understand the mechanism
  - Address translation: mapping from a virtual address to a physical address through a page table
    - Finding the page table index to locate a page table entry
    - Checking the valid bit
    - Extracting the physical frame number
    - Combining it with the offset from the address to create the physical address
- Page faults
  - because the valid bit is false
  - because the page table entry doesn't permit access of the type being attempted (read, write, execute)
- Holes in the virtual address space
  - How?
  - Why?

## Processes / Address Spaces

- How do you create a process?
  - What has to happen to create a process?
  - What doesn't happen?
    - Role of exec()
- Memory layout for process address space
  - Stack / heap / static data / text
  - Permissions for each section of the address space
- Process control blocks and process state
  - Running / runnable / blocked
- The context switch mechanism
  - Why must the hardware save the PC at the time of an interrupt / trap / exception? Why can't the software do it?
- Copy on write
  - What is it used for?
  - How does it work?

## Threads

- Why arent' processes enough?
- Creating a thread (in Java)
- join()'ing with a thread
- Why use both kernel threads and user level threads?

## Threads / Synchronization

- Recognizing code that has a race condition
- Recognizing code that is a critical section
  - And code that isn't
- Resolving critical sections through mutual exclusion
  - In general, using locks
  - In Java, using "synchronized" methods
- Recognizing when deadlock might occur
- Can multiple processes have arace condition (among them)? Can they deadlock?

## Limits to Parallelism / Concurrency

- Number of cores on system
- Number of threads in the application
- Depth of pipeline
- Dependences among instructions (RAW, WAW, WAR)
- Amdahl's Law

